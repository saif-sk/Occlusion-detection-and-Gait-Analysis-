{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e4d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import time\n",
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from imutils.video import FileVideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe22a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r\"C:\\Users\\user\\Desktop\\caop\\data\\trail3.mp4\"\n",
    "fvs = FileVideoStream(video_path, queue_size=1024).start()  \n",
    "time.sleep(1.0)\n",
    "kernelSize = 7\n",
    "backgroundHistory = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e12c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_contour_area = 500  # minimum area of detected contours to consider as occlusions\n",
    "kernel_size = 15  # size of the filter kernel for the morphological operations\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "_, prev_frame = cap.read()\n",
    "\n",
    "cv2.namedWindow(\"Occlusion Detection\", cv2.WINDOW_NORMAL)  # Create a resizable window\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frames to grayscale\n",
    "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_curr = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply background subtraction to get foreground mask\n",
    "    fg_mask = cv2.absdiff(gray_curr, gray_prev)\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to remove noise and fill holes in the foreground mask\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "    fg_mask = cv2.dilate(fg_mask, kernel, iterations=2)\n",
    "    fg_mask = cv2.erode(fg_mask, kernel, iterations=2)\n",
    "\n",
    "    # Find contours in the foreground mask\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    occlusions_detected = False\n",
    "    for contour in contours:\n",
    "        # Check if the contour area is greater than the minimum threshold\n",
    "        if cv2.contourArea(contour) > min_contour_area:\n",
    "            # Draw a bounding box around the contour\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "            occlusions_detected = True\n",
    "\n",
    "    # Resize the output window to a width of 800 pixels and a height of 600 pixels\n",
    "    cv2.resizeWindow(\"Occlusion Detection\", 800, 600)\n",
    "\n",
    "    # Show the current frame with bounding boxes around detected occlusions\n",
    "    cv2.imshow(\"Occlusion Detection\", frame)\n",
    "    if cv2.waitKey(30) == ord('q'):\n",
    "        break\n",
    "\n",
    "    prev_frame = frame.copy()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214c26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openposeProtoFile = r\"C:\\Users\\user\\Desktop\\caop\\pose_deploy_linevec.prototxt\"\n",
    "openposeWeightsFile = r\"C:\\Users\\user\\Desktop\\caop\\pose_iter_440000.caffemodel\"\n",
    "nPoints = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304934c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectdetectionProtoFile = r\"C:\\Users\\user\\Desktop\\caop\\MobileNetSSD_deploy.prototxt\"\n",
    "objectdetectionWeightsFile = r\"C:\\Users\\user\\Desktop\\caop\\MobileNetSSD_deploy.caffemodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c99e9c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Output Format\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank',\n",
    "                    'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
    "\n",
    "POSE_PAIRS = [[1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7],\n",
    "              [1, 8], [8, 9], [9, 10], [1, 11], [11, 12], [12, 13],\n",
    "              [1, 0], [0, 14], [14, 16], [0, 15], [15, 17],\n",
    "              [2, 17], [5, 16]]\n",
    "# index of pafs correspoding to the POSE_PAIRS\n",
    "# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\n",
    "\n",
    "mapIdx = [[31, 32], [39, 40], [33, 34], [35, 36], [41, 42], [43, 44],\n",
    "          [19, 20], [21, 22], [23, 24], [25, 26], [27, 28], [29, 30],\n",
    "          [47, 48], [49, 50], [53, 54], [51, 52], [55, 56],\n",
    "          [37, 38], [45, 46]]\n",
    "\n",
    "colors = [[0, 100, 255], [0, 100, 255], [0, 255, 255], [0, 100, 255], [0, 255, 255], [0, 100, 255],\n",
    "          [0, 255, 0], [255, 200, 100], [255, 0, 255], [0, 255, 0], [255, 200, 100], [255, 0, 255],\n",
    "          [0, 0, 255], [255, 0, 0], [200, 200, 0], [255, 0, 0], [200, 200, 0], [0, 0, 0]]\n",
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "           \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "\n",
    "def getKeypoints(prob_map, thres=0.1):\n",
    "    map_smooth = cv2.GaussianBlur(prob_map, (3, 3), 0, 0)\n",
    "\n",
    "    map_mask = np.uint8(map_smooth > thres)\n",
    "    keypoints_array = []\n",
    "# find the blobs\n",
    "    contours, _ = cv2.findContours(map_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    \n",
    "    for cnt in contours:\n",
    "        blob_mask = np.zeros(map_mask.shape)\n",
    "        blob_mask = cv2.fillConvexPoly(blob_mask, cnt, 1)\n",
    "        masked_prob_map = map_smooth * blob_mask\n",
    "        _, max_val, _, max_loc = cv2.minMaxLoc(masked_prob_map)\n",
    "        keypoints_array.append(max_loc + (prob_map[max_loc[1], max_loc[0]],))\n",
    "\n",
    "    return keypoints_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c027dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find valid connections between the different joints of a all persons present\n",
    "def getValidPairs(generated_output):\n",
    "    validpairs = []\n",
    "    invalidpairs = []\n",
    "    n_interp_samples = 10\n",
    "    paf_score_th = 0.1\n",
    "    conf_th = 0.7\n",
    "     # loop for every POSE_PAIR\n",
    "    for k in range(len(mapIdx)):\n",
    "        \n",
    "        pafA = generated_output[0, mapIdx[k][0], :, :]\n",
    "        pafB = generated_output[0, mapIdx[k][1], :, :]\n",
    "        pafA = cv2.resize(pafA, (frameWidth, frameHeight))\n",
    "        pafB = cv2.resize(pafB, (frameWidth, frameHeight))\n",
    "\n",
    "        # Find the keypoints for the first and second limb\n",
    "        candA = detected_keypoints[POSE_PAIRS[k][0]]\n",
    "        candB = detected_keypoints[POSE_PAIRS[k][1]]\n",
    "        nA = len(candA)\n",
    "        nB = len(candB)\n",
    "\n",
    "        # If keypoints for the joint-pair is detected\n",
    "        # check every joint in candA with every joint in candB\n",
    "        # Calculate the distance vector between the two joints\n",
    "        # Find the PAF values at a set of interpolated points between the joints\n",
    "        # Use the above formula to compute a score to mark the connection valid\n",
    "\n",
    "        if nA != 0 and nB != 0:\n",
    "            valid_pair = np.zeros((0, 3))\n",
    "            for i in range(nA):\n",
    "                max_j = -1\n",
    "                max_score = -1\n",
    "                found = 0\n",
    "                for j in range(nB):\n",
    "                    # Find d_ij\n",
    "                    d_ij = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                    norm = np.linalg.norm(d_ij)\n",
    "                    if norm:\n",
    "                        d_ij = d_ij / norm\n",
    "                    else:\n",
    "                        continue\n",
    "                   \n",
    "                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),\n",
    "                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))\n",
    "                    \n",
    "                    paf_interp = []\n",
    "                    for k in range(len(interp_coord)):\n",
    "                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],\n",
    "                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))]])\n",
    "                    \n",
    "                    paf_scores = np.dot(paf_interp, d_ij)\n",
    "                    avg_paf_score = sum(paf_scores) / len(paf_scores)\n",
    "\n",
    "                    # Check if the connection is valid\n",
    "                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair\n",
    "                    if (len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples) > conf_th:\n",
    "                        if avg_paf_score > max_score:\n",
    "                            max_j = j\n",
    "                            max_score = avg_paf_score\n",
    "                            found = 1\n",
    "                # Append the connection to the list\n",
    "                if found:\n",
    "                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], max_score]], axis=0)\n",
    "\n",
    "           # Append the detected connections to the global list\n",
    "            validpairs.append(valid_pair)\n",
    "        else:  # If no keypoints are detected\n",
    "            invalidpairs.append(k)\n",
    "            validpairs.append([])\n",
    "    return validpairs, invalidpairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ecb97a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0329c74a3157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mfvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mframeClone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\imutils\\convenience.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, width, height, inter)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# grab the image size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# if both the width and height are None, then return the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# This function creates a list of keypoints belonging to each person\n",
    "# For each detected valid pair, it assigns the joint(s) to a person\n",
    "\n",
    "def getPersonwiseKeypoints(validpairs, invalidpairs):\n",
    "    \n",
    "    personwise_keypoints = -1 * np.ones((0, 19))\n",
    "\n",
    "    for k in range(len(mapIdx)):\n",
    "        if k not in invalidpairs:\n",
    "            partAs = validpairs[k][:, 0]\n",
    "            partBs = validpairs[k][:, 1]\n",
    "            indexA, indexB = np.array(POSE_PAIRS[k])\n",
    "\n",
    "            for i in range(len(validpairs[k])):\n",
    "                found = 0\n",
    "                person_idx = -1\n",
    "                for j in range(len(personwise_keypoints)):\n",
    "                    if personwise_keypoints[j][indexA] == partAs[i]:\n",
    "                        person_idx = j\n",
    "                        found = 1\n",
    "                        break\n",
    "\n",
    "                if found:\n",
    "                    personwise_keypoints[person_idx][indexB] = partBs[i]\n",
    "                    personwise_keypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + validpairs[k][i][\n",
    "                        2]\n",
    "\n",
    "    \n",
    "                elif not found and k < 17:\n",
    "                    row = -1 * np.ones(19)\n",
    "                    row[indexA] = partAs[i]\n",
    "                    row[indexB] = partBs[i]\n",
    "                    row[-1] = sum(keypoints_list[validpairs[k][i, :2].astype(int), 2]) + validpairs[k][i][2]\n",
    "                    personwise_keypoints = np.vstack([personwise_keypoints, row])\n",
    "    return personwise_keypoints\n",
    "\n",
    "while fvs.more():\n",
    "    frame = fvs.read()\n",
    "    frame = imutils.resize(frame, width=500)\n",
    "\n",
    "    frameClone = frame.copy()\n",
    "\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "   # Fix the input Height and get the width according to the Aspect Ratio\n",
    "    inHeight = 368\n",
    "    inWidth = int((inHeight / frameHeight) * frameWidth)\n",
    "    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    net = cv2.dnn.readNetFromCaffe(openposeProtoFile, openposeWeightsFile)\n",
    "    objnet = cv2.dnn.readNetFromCaffe(objectdetectionProtoFile, objectdetectionWeightsFile)\n",
    "\n",
    "    net.setInput(inpBlob)\n",
    "    output = net.forward()\n",
    "\n",
    "    \n",
    "    objnet.setInput(inpBlob)\n",
    "    detections = objnet.forward()\n",
    "\n",
    "   \n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "         # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > 0.6:\n",
    "            \n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([frameWidth, frameHeight, frameWidth, frameHeight])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            \n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frameClone, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "     # Applying background subtraction on the capture frame\n",
    "    # frame = fgbg.apply(frame)\n",
    "\n",
    "    detected_keypoints = []\n",
    "    keypoints_list = np.zeros((0, 3))\n",
    "    keypoint_id = 0\n",
    "    threshold = 0.1\n",
    "\n",
    "    for part in range(nPoints):\n",
    "        probMap = output[0, part, :, :]\n",
    "        probMap = cv2.resize(probMap, (frame.shape[1], frame.shape[0]))\n",
    "        keypoints = getKeypoints(probMap, threshold)\n",
    "\n",
    "        keypoints_with_id = []\n",
    "        for i in range(len(keypoints)):\n",
    "            keypoints_with_id.append(keypoints[i] + (keypoint_id,))\n",
    "            keypoints_list = np.vstack([keypoints_list, keypoints[i]])\n",
    "            keypoint_id += 1\n",
    "\n",
    "        detected_keypoints.append(keypoints_with_id)\n",
    "\n",
    "    \n",
    "        for i in range(min(nPoints, len(detected_keypoints))):\n",
    "            for j in range(len(detected_keypoints[i])):\n",
    "                cv2.circle(frame, detected_keypoints[i][j][0:2], 5, colors[i], -1, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Keypoints\", frame)\n",
    "\n",
    "    valid_pairs, invalid_pairs = getValidPairs(output)\n",
    "    personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)\n",
    "\n",
    "    for i in range(17):\n",
    "        for n in range(len(personwiseKeypoints)):\n",
    "            index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]\n",
    "            if -1 in index:\n",
    "                continue\n",
    "            B = np.int32(keypoints_list[index.astype(int), 0])\n",
    "            A = np.int32(keypoints_list[index.astype(int), 1])\n",
    "            cv2.line(frame, (B[0], A[0]), (B[1], A[1]), colors[i], 2, cv2.LINE_AA)\n",
    "\n",
    "    # frame = cv2.addWeighted(frameClone, 0.5, frame, 0.5, 0.0)\n",
    "    frame = cv2.addWeighted(frameClone, 0.5, frame, 0.5, 0.0)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    k = cv2.waitKey(10) & 0xff\n",
    "    if k == 15:\n",
    "        break\n",
    "\n",
    "\n",
    "fvs.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab03a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1256b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1cbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ff0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf0f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
